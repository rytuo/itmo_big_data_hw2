# HW02 spark

[Код домашнего задания](./notebooks/hw_spark.ipynb)

[Папка со скринами](./notebooks/img/)

### bigdata-docker-compose
(Почти) настроенный докер с последним hadoop и сопутствующими инструментами на борту

Образ при сборке выкачивает много данных (ставит хадупы\юпитеры\хайвы и т.д.). Это норма.
Лучше не запускаться при подключении к лимитному интернету.

Для запуска:

1. Поставить docker + docker-compose на локальную машину

Для запуска hadoop:
1. Сначала запускаем неймноду с командой command: ["hdfs", "namenode", "-format", "-force"] 
2. Так запуститься надо только в первый раз (либо, после того, как вы снесли образ и примонтированный раздел)
3. После того, как контейнер отработал и завершился, запускаемся с командой command: ["hdfs", "namenode"]
4. После неймноды поднимаем датаноды, нодменеджеры и т.д.

Для запуска hive:
1. Сначала поднимаем постгрес.
1. Затем поднимаем метастор с командой command: ["schematool", "--dbType", "postgres", "--initSchema"]
2. Так запуститься надо только в первый раз (либо, после того, как вы снесли образ и примонтированный раздел)
2. После того, как контейнер отработал и завершился, запускаемся с командой command: [ "hive", "--service", "metastore" ]
3. После метастора запускаем hiveserver2
